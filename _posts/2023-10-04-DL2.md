---
title: Deep learning 2 - Logistic Regression as a Neural Network
date: 2023-09-27 15:30:00 +/-TTTT
categories: [Deep Learning]
tags: [deep learning, neural network, deep learning coursera, logistic regression, binary classification]
toc: true
math: true
comments: true
published: true
img_path: /pic/DL2/
---

# Neural Networks Basics


Bài viết này chúng ta sẽ tập trung vào những kiến ​​thức cơ bản về lập trình mạng nơ-ron (Neural network), đặc biệt là một số kỹ thuật quan trọng, chẳng hạn như cách xử lý các ví dụ huấn luyện m trong tính toán và cách triển khai quá trình lan truyền tiến (forward propagation) và lan truyền ngược (Backward propagation). 

## Logistic Regression

Logistic Regression (Hồi quy logistic) là một kỹ thuật khác được học máy mượn từ lĩnh vực thống kê.

Đây là phương pháp phù hợp cho các bài toán phân loại nhị phân (các bài toán có hai giá trị lớp). Và Logistic Regression là cách đơn giản để cho chúng ta thấy cách hoạt động neural network.

**Logistic Regression Cost Function**

Trong hồi quy logistic, chúng ta muốn huấn luyện các tham số `w` và `b`, chúng ta cần xác định hàm chi phí.

$$\hat{y} = \sigma(w^{T}x^{(i)}+b)$$

$$\text{Trong đó: } \sigma(z^{(i)}) = \frac{1}{1 + e^{-z^{(i)}}}$$

$$\text{Dữ liệu:} \{(x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)})$$

$$\text{Mục tiêu: } \hat{y}^{(i)} ≈ y^{(i)}$$

Nếu đây là lần đầu tiên bạn tiếp cận với Logistic Regression thì bạn có thể xem bài giảng về LR của tôi tại đây [Logistic Regression lecture](https://github.com/AppSalmon/Machine-learning-lecture-of-AI-Faster-team/tree/main/Lecture07_Logistic_Regression) hoặc đọc blog này [machinelearningcoban.com/2017/01/27/logisticregression](https://machinelearningcoban.com/2017/01/27/logisticregression/), tôi tin là sau khi đọc xong bạn sẽ hiểu vẻ đẹp của Logistic Regression.

Loss Function là hàm mất mát đo sự chệnh lệch giữa y dự đoán $$\hat{y}^{i}$$ và y thực tế $$y^{i}$$, nói đơn giản hơn thì nó tính lỗi (chi phí) cho một ví dụ (sample) huấn luyện.

$$L(\hat{y}^{(i)}, y^{(i)}) = \frac{1}{2}(\hat{y}^{(i)} - y^{(i)})^{2}$$

$$L(ŷ(i), y(i)) = -(y(i) \log(ŷ(i)) + (1 - y(i)) \log(1 - ŷ(i)))$$

$$\begin{cases}
L(ŷ, y) = \log(ŷ) & \text{if } y = 1 \\
L(ŷ, y) = \log(1 - ŷ) & \text{if } y = 0
\end{cases}$$

Cost function là tính toán trung bình tất cả lỗi, chi phí của cả tập huấn lệnh, và mục tiêu của chúng ta là tìm `w` và `b` để Cost function là nhỏ nhất cho tập huấn lệnh.

$$\begin{equation}
J(w, b) = \frac{1}{m} \sum_{i=1}^m L(\hat{y}^{(i)}, y^{(i)}) = -\frac{1}{m} \sum_{i=1}^m [(y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)})\log(1 - \hat{y}^{(i)})]
\end{equation}$$

Ngắn gọn hơn:

$$J(w, b) = -\sum_{i=1}^m \log(\hat{y}^{(i)})^{y^{(i)}} (1 - \hat{y}^{(i)})^{1 - y^{(i)}}$$

Hàm mất mát (Loss function) đo lường mức độ hiệu quả của mô hình trên một ví dụ đào tạo, trong khi hàm chi phí (Cost function) đo lường mức độ hiệu quả của mô hình (cụ thể là các tham số `w` và `b`) trên toàn bộ tập huấn luyện.


## Gradient Descent

Khi chúng ta học bất kỳ khóa học nào về Machine learning hoặc Deep learning thì Gradient Descent sẽ là thứ xuất hiện phổ biến nhất, bạn nên master kỹ thuật này, Nó được sử dụng khi train model, khi mà các cost function quá phức tạp để tìm `w` và `b` tối ưu, thì Gradient Descent xuất hiện, nó có thể kết hợp với mọi thuật toán một cách dễ hiểu và dễ thực hiện.

Tôi đã có những slide và bài tập về Gradient Descent và bạn có thể xem [ở đây](https://github.com/AppSalmon/Machine-learning-lecture-of-AI-Faster-team/tree/main/Lecture03_Gradient_Descent), Ngoài ra bạn cũng có thể tham khảo ở đây: [GD1](https://machinelearningcoban.com/2017/01/12/gradientdescent/), [GD2](https://machinelearningcoban.com/2017/01/16/gradientdescent2/)

Nói ngắn gọn về ý tưởng của Gradient Descent là:
- Khởi tạo các tham số `w`, `b` ngẫu nhiên (thường thì xấp xỉ 0).
- Tính Cost and Gradient cho tập training với tham số `w`, `b`.
- Update các tham số `w` và `b` `với learning rate` đặt trước: 

$$w_{new} = w_{old} – lr * gradient_of_at(w_{old})$$ 

làm tương tự với `b`.

- Lặp lại các bước này cho đến khi bạn đạt được giá trị tối thiểu của Cost function, từ đó bạn sẽ có được `w` và `b` tối ưu.

![gradient-descent.jpeg](gradient-descent.jpeg)


## Tham khảo

Bài viết dựa trên khóa học Deep Learning Specialization Coursera nổi tiếng của Andrew Ng - [Link](https://www.coursera.org/specializations/deep-learning).


## Bình luận & thảo luận

Cảm ơn bạn đã dành thời gian để đọc, hãy trò chuyện và góp ý với mình ở dưới hoặc vào bằng <a href = "https://forms.gle/ZUrzUFKadCJBAEzaA"> link </a>.

<iframe src="https://docs.google.com/forms/d/e/1FAIpQLSdYX6124QWR49d27Gu08whQH9MhDvXeW9o4KkA-kblLt4URwA/viewform?embedded=true" width="640" height="686" frameborder="0" marginheight="0" marginwidth="0">Đang tải…</iframe>
